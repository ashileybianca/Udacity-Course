{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading Models\n",
    "In this notebook, I'll show you how to save and load models with PyTorch. This is important because you'll often want to load previously trained models to use in making predictions or to continue training on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import helper\n",
    "import fc_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 389,
       "width": 389
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Train a network\n",
    "\n",
    "To make things more concise here, I moved the model architecture and training code from the last part to a file called fc_model.\n",
    "\n",
    "Importing this, we can easily create a fully-connected network with fc_model.Network, and train the network using fc_model.train.\n",
    "\n",
    "I'll use this model (once it's trained) to demonstrate how we can save and load models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network, define the criterion and optimizer\n",
    "\n",
    "model = fc_model.Network(784, 10, [512, 256, 128])\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2..  Training Loss: 1.684..  Test Loss: 0.950..  Test Accuracy: 0.642\n",
      "Epoch: 1/2..  Training Loss: 1.027..  Test Loss: 0.705..  Test Accuracy: 0.737\n",
      "Epoch: 1/2..  Training Loss: 0.875..  Test Loss: 0.664..  Test Accuracy: 0.745\n",
      "Epoch: 1/2..  Training Loss: 0.780..  Test Loss: 0.643..  Test Accuracy: 0.747\n",
      "Epoch: 1/2..  Training Loss: 0.742..  Test Loss: 0.618..  Test Accuracy: 0.760\n",
      "Epoch: 1/2..  Training Loss: 0.717..  Test Loss: 0.593..  Test Accuracy: 0.771\n",
      "Epoch: 1/2..  Training Loss: 0.684..  Test Loss: 0.584..  Test Accuracy: 0.772\n",
      "Epoch: 1/2..  Training Loss: 0.652..  Test Loss: 0.569..  Test Accuracy: 0.785\n",
      "Epoch: 1/2..  Training Loss: 0.647..  Test Loss: 0.559..  Test Accuracy: 0.791\n",
      "Epoch: 1/2..  Training Loss: 0.681..  Test Loss: 0.571..  Test Accuracy: 0.784\n",
      "Epoch: 1/2..  Training Loss: 0.642..  Test Loss: 0.534..  Test Accuracy: 0.803\n",
      "Epoch: 1/2..  Training Loss: 0.614..  Test Loss: 0.536..  Test Accuracy: 0.801\n",
      "Epoch: 1/2..  Training Loss: 0.617..  Test Loss: 0.527..  Test Accuracy: 0.806\n",
      "Epoch: 1/2..  Training Loss: 0.609..  Test Loss: 0.529..  Test Accuracy: 0.812\n",
      "Epoch: 1/2..  Training Loss: 0.599..  Test Loss: 0.510..  Test Accuracy: 0.811\n",
      "Epoch: 1/2..  Training Loss: 0.578..  Test Loss: 0.519..  Test Accuracy: 0.809\n",
      "Epoch: 1/2..  Training Loss: 0.600..  Test Loss: 0.512..  Test Accuracy: 0.815\n",
      "Epoch: 1/2..  Training Loss: 0.589..  Test Loss: 0.497..  Test Accuracy: 0.821\n",
      "Epoch: 1/2..  Training Loss: 0.603..  Test Loss: 0.486..  Test Accuracy: 0.826\n",
      "Epoch: 1/2..  Training Loss: 0.575..  Test Loss: 0.506..  Test Accuracy: 0.814\n",
      "Epoch: 1/2..  Training Loss: 0.575..  Test Loss: 0.487..  Test Accuracy: 0.820\n",
      "Epoch: 1/2..  Training Loss: 0.586..  Test Loss: 0.489..  Test Accuracy: 0.812\n",
      "Epoch: 1/2..  Training Loss: 0.578..  Test Loss: 0.500..  Test Accuracy: 0.811\n",
      "Epoch: 2/2..  Training Loss: 0.586..  Test Loss: 0.477..  Test Accuracy: 0.827\n",
      "Epoch: 2/2..  Training Loss: 0.576..  Test Loss: 0.483..  Test Accuracy: 0.823\n",
      "Epoch: 2/2..  Training Loss: 0.572..  Test Loss: 0.496..  Test Accuracy: 0.819\n",
      "Epoch: 2/2..  Training Loss: 0.525..  Test Loss: 0.484..  Test Accuracy: 0.823\n",
      "Epoch: 2/2..  Training Loss: 0.549..  Test Loss: 0.477..  Test Accuracy: 0.825\n",
      "Epoch: 2/2..  Training Loss: 0.526..  Test Loss: 0.479..  Test Accuracy: 0.824\n",
      "Epoch: 2/2..  Training Loss: 0.555..  Test Loss: 0.462..  Test Accuracy: 0.827\n",
      "Epoch: 2/2..  Training Loss: 0.523..  Test Loss: 0.483..  Test Accuracy: 0.821\n",
      "Epoch: 2/2..  Training Loss: 0.520..  Test Loss: 0.459..  Test Accuracy: 0.831\n",
      "Epoch: 2/2..  Training Loss: 0.529..  Test Loss: 0.462..  Test Accuracy: 0.832\n",
      "Epoch: 2/2..  Training Loss: 0.519..  Test Loss: 0.449..  Test Accuracy: 0.835\n",
      "Epoch: 2/2..  Training Loss: 0.528..  Test Loss: 0.467..  Test Accuracy: 0.830\n",
      "Epoch: 2/2..  Training Loss: 0.580..  Test Loss: 0.503..  Test Accuracy: 0.812\n",
      "Epoch: 2/2..  Training Loss: 0.549..  Test Loss: 0.452..  Test Accuracy: 0.831\n",
      "Epoch: 2/2..  Training Loss: 0.495..  Test Loss: 0.449..  Test Accuracy: 0.835\n",
      "Epoch: 2/2..  Training Loss: 0.521..  Test Loss: 0.445..  Test Accuracy: 0.837\n",
      "Epoch: 2/2..  Training Loss: 0.521..  Test Loss: 0.440..  Test Accuracy: 0.838\n",
      "Epoch: 2/2..  Training Loss: 0.533..  Test Loss: 0.455..  Test Accuracy: 0.832\n",
      "Epoch: 2/2..  Training Loss: 0.524..  Test Loss: 0.457..  Test Accuracy: 0.837\n",
      "Epoch: 2/2..  Training Loss: 0.524..  Test Loss: 0.443..  Test Accuracy: 0.837\n",
      "Epoch: 2/2..  Training Loss: 0.517..  Test Loss: 0.459..  Test Accuracy: 0.835\n",
      "Epoch: 2/2..  Training Loss: 0.539..  Test Loss: 0.454..  Test Accuracy: 0.833\n",
      "Epoch: 2/2..  Training Loss: 0.538..  Test Loss: 0.465..  Test Accuracy: 0.831\n"
     ]
    }
   ],
   "source": [
    "fc_model.train(model, trainloader, testloader, criterion, optimizer, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading networks\n",
    "\n",
    "As you can imagine, it's impractical to train a network every time you need to use it. Instead, we can save trained networks then load them later to train more or use them for predictions.\n",
    "\n",
    "The parameters for PyTorch networks are stored in a model's `state_dict`. We can see the state dict contains the weight and bias matrices for each of our layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model: \n",
      "\n",
      " Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ") \n",
      "\n",
      "The state dict keys: \n",
      "\n",
      " odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n",
      "tensor([-0.0189,  0.0107, -0.0131, -0.0553, -0.0211, -0.0513, -0.0444, -0.0562,\n",
      "        -0.0213,  0.0024,  0.0064, -0.0060,  0.0028, -0.0184, -0.0055, -0.0428,\n",
      "        -0.0228, -0.0374, -0.0292, -0.0104, -0.0053, -0.0189, -0.0145, -0.0504,\n",
      "        -0.0215, -0.0503, -0.0200, -0.0391, -0.0285, -0.0117, -0.0111, -0.0055,\n",
      "         0.0186,  0.0212, -0.0218, -0.0335, -0.0296, -0.0201, -0.0116, -0.0020,\n",
      "        -0.0296, -0.0228, -0.0160, -0.0069, -0.0572,  0.0066,  0.0089, -0.0285,\n",
      "        -0.0568, -0.0531, -0.0196, -0.0378,  0.0013, -0.0672, -0.0488, -0.0410,\n",
      "        -0.0160, -0.0297,  0.0103, -0.0101, -0.0212, -0.0271, -0.0137, -0.0405,\n",
      "        -0.0158, -0.0404, -0.0516, -0.0045, -0.0005, -0.0492, -0.0017,  0.0240,\n",
      "        -0.0102, -0.0342,  0.0017, -0.0251,  0.0115,  0.0035,  0.0187, -0.0607,\n",
      "        -0.0235, -0.0323, -0.0139, -0.0260, -0.0507,  0.0178,  0.0036, -0.0038,\n",
      "         0.0065, -0.0371, -0.0128, -0.0184, -0.0345,  0.0059, -0.0223, -0.0454,\n",
      "        -0.0388,  0.0248,  0.0191, -0.0236, -0.0038,  0.0113,  0.0398, -0.0280,\n",
      "        -0.0261, -0.0019, -0.0001,  0.0090,  0.0078, -0.0234, -0.0536, -0.0509,\n",
      "         0.0079, -0.0081, -0.0528, -0.0152, -0.0419, -0.0159, -0.0318, -0.0258,\n",
      "        -0.0331, -0.0502,  0.0068, -0.0367, -0.0061, -0.0680,  0.0029, -0.0596,\n",
      "        -0.0322, -0.0488, -0.0026, -0.0475, -0.0120, -0.0489, -0.0021,  0.0312,\n",
      "        -0.0243, -0.0025, -0.0432, -0.0547,  0.0102,  0.0075, -0.0483,  0.0092,\n",
      "        -0.0049, -0.0205, -0.0064, -0.0427, -0.0276, -0.0539, -0.0047,  0.0006,\n",
      "        -0.0318, -0.0322, -0.0427, -0.0321, -0.0587, -0.0192,  0.0283, -0.0358,\n",
      "         0.0142, -0.0037, -0.0048, -0.0392, -0.0314, -0.0034,  0.0038, -0.0008,\n",
      "        -0.0103, -0.0436,  0.0429,  0.0227,  0.0077,  0.0099, -0.0131,  0.0103,\n",
      "         0.0033, -0.0242, -0.0208,  0.0217, -0.0104,  0.0156,  0.0020,  0.0121,\n",
      "         0.0007, -0.0044,  0.0085,  0.0216, -0.0768,  0.0183, -0.0416, -0.0177,\n",
      "        -0.0221, -0.0018, -0.0254, -0.0449, -0.0536,  0.0134, -0.0003, -0.0167,\n",
      "         0.0142, -0.0214, -0.0421,  0.0055, -0.0076, -0.0618, -0.0278, -0.0049,\n",
      "        -0.0501, -0.0467, -0.0106,  0.0123, -0.0409, -0.0209, -0.0349, -0.0343,\n",
      "        -0.0376, -0.0090, -0.0481, -0.0054, -0.0135,  0.0043,  0.0039,  0.0090,\n",
      "         0.0103, -0.0225, -0.0149,  0.0146,  0.0174,  0.0030, -0.0374, -0.0229,\n",
      "         0.0085,  0.0187, -0.0399, -0.0343, -0.0092,  0.0231, -0.0074,  0.0110,\n",
      "        -0.0517, -0.0381, -0.0029, -0.0291,  0.0110,  0.0117,  0.0008, -0.0052,\n",
      "         0.0245, -0.0755, -0.0248, -0.0384, -0.0354, -0.0108,  0.0083, -0.0343,\n",
      "        -0.0428,  0.0121, -0.0312, -0.0542, -0.0129, -0.0370, -0.0086, -0.0523,\n",
      "         0.0153,  0.0168, -0.0327, -0.0199,  0.0013, -0.0429, -0.0270, -0.0306,\n",
      "         0.0168, -0.0399,  0.0409, -0.0419, -0.0077,  0.0174,  0.0051, -0.0237,\n",
      "        -0.0163, -0.0084, -0.0693, -0.0140,  0.0065, -0.0216, -0.0181,  0.0156,\n",
      "        -0.0280, -0.0544, -0.0396,  0.0350,  0.0379,  0.0109, -0.0688, -0.0196,\n",
      "        -0.0271,  0.0226, -0.0490, -0.0145, -0.0090,  0.0206,  0.0149, -0.0275,\n",
      "        -0.0360, -0.0063, -0.0244, -0.0048, -0.0165, -0.0471,  0.0025,  0.0030,\n",
      "        -0.0189,  0.0336, -0.0424, -0.0228, -0.0099, -0.0384,  0.0019,  0.0068,\n",
      "        -0.0137, -0.0447, -0.0200, -0.0306,  0.0127, -0.0134, -0.0387, -0.0609,\n",
      "        -0.0508,  0.0164,  0.0104, -0.0094, -0.0035, -0.0472, -0.0282, -0.0042,\n",
      "        -0.0089, -0.0452, -0.0473,  0.0112,  0.0607,  0.0034, -0.0063, -0.0404,\n",
      "         0.0100, -0.0145,  0.0099, -0.0030, -0.0055, -0.0207, -0.0133,  0.0062,\n",
      "        -0.0043,  0.0120,  0.0069, -0.0452, -0.0247, -0.0360, -0.0278,  0.0254,\n",
      "        -0.0239, -0.0405,  0.0116, -0.0475, -0.0447, -0.0587,  0.0199, -0.0431,\n",
      "        -0.0137, -0.0455, -0.0111, -0.0107, -0.0158, -0.0437, -0.0216, -0.0148,\n",
      "        -0.0308, -0.0403, -0.0194, -0.0456, -0.0181, -0.0035, -0.0327, -0.0503,\n",
      "        -0.0338,  0.0070, -0.0372, -0.0279, -0.0090, -0.0295, -0.0294,  0.0091,\n",
      "        -0.0037,  0.0032, -0.0434, -0.0390,  0.0175, -0.0330, -0.0142, -0.0287,\n",
      "         0.0071, -0.0256,  0.0187, -0.0203, -0.0225, -0.0467,  0.0065, -0.0465,\n",
      "         0.0221,  0.0310, -0.0367, -0.0285, -0.0219, -0.0131, -0.0318, -0.0329,\n",
      "        -0.0150, -0.0357, -0.0713, -0.0171, -0.0179, -0.0327,  0.0069, -0.0591,\n",
      "        -0.0097, -0.0080, -0.0331, -0.0185, -0.0389, -0.0670,  0.0004, -0.0131,\n",
      "        -0.0614, -0.0522, -0.0230,  0.0062, -0.0202,  0.0119,  0.0011, -0.0472,\n",
      "        -0.0288, -0.0310, -0.0451, -0.0309, -0.0396, -0.0162,  0.0321, -0.0011,\n",
      "         0.0098, -0.0209, -0.0162, -0.0073, -0.0061, -0.0051, -0.0213, -0.0322,\n",
      "         0.0091, -0.0156, -0.0151, -0.0101, -0.0237,  0.0617, -0.0080, -0.0311,\n",
      "        -0.0220, -0.0129, -0.0117,  0.0056, -0.0376, -0.0202, -0.0346, -0.0199,\n",
      "        -0.0031,  0.0441,  0.0230, -0.0135, -0.0506, -0.0199, -0.0244, -0.0415,\n",
      "        -0.0130, -0.0080, -0.0286, -0.0490, -0.0123, -0.0418,  0.0016, -0.0315,\n",
      "        -0.0179,  0.0363, -0.0427,  0.0195, -0.0088, -0.0523, -0.0333, -0.0066,\n",
      "        -0.0144, -0.0121, -0.0475, -0.0205,  0.0446, -0.0145, -0.0285,  0.0101,\n",
      "         0.0105, -0.0420,  0.0044, -0.0398, -0.0547, -0.0084, -0.0722, -0.0130]) tensor([[ 0.0459, -0.0114,  0.0215,  ...,  0.0331,  0.0180,  0.0258],\n",
      "        [-0.0050,  0.0089,  0.0291,  ..., -0.0060,  0.0238,  0.0319],\n",
      "        [ 0.0108,  0.0460,  0.0336,  ..., -0.0358,  0.0200,  0.0478],\n",
      "        ...,\n",
      "        [ 0.0182, -0.0389,  0.0109,  ...,  0.0127, -0.0018, -0.0044],\n",
      "        [ 0.0252,  0.0737,  0.0418,  ...,  0.0014,  0.0317,  0.0798],\n",
      "        [ 0.0568,  0.0104,  0.0376,  ...,  0.0060,  0.0079,  0.0017]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Our model: \\n\\n\", model, '\\n')\n",
    "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())\n",
    "\n",
    "hidden_layer1_weights = model.state_dict()['hidden_layers.0.weight']\n",
    "hidden_layer1_biases = model.state_dict()['hidden_layers.0.bias']\n",
    "\n",
    "print(hidden_layer1_biases,  hidden_layer1_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The simplest thing to do is simply save the state dict with torch.save. For example, we can save it to a file 'checkpoint.pth'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O estado do modelo é salvo em um arquivo usando torch.save, e posteriormente pode ser carregado usando torch.load. (criou-se um arquivo checkpoint.pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then we can load the state dict with torch.load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('checkpoint.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No entanto, é importante notar que o carregamento do state_dict funciona apenas se a arquitetura do modelo for exatamente a mesma que a arquitetura no momento em que o estado foi salvo. Caso contrário, ocorrerá um erro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param with shape torch.Size([512, 784]) from checkpoint, the shape in current model is torch.Size([400, 784]).\n\tsize mismatch for hidden_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([400]).\n\tsize mismatch for hidden_layers.1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([200, 400]).\n\tsize mismatch for hidden_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([100, 200]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 128]) from checkpoint, the shape in current model is torch.Size([10, 100]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m fc_model\u001b[38;5;241m.\u001b[39mNetwork(\u001b[38;5;241m784\u001b[39m, \u001b[38;5;241m10\u001b[39m, [\u001b[38;5;241m400\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m100\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# This will throw an error because the tensor sizes are wrong!\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1497\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1492\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1493\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1494\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1498\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param with shape torch.Size([512, 784]) from checkpoint, the shape in current model is torch.Size([400, 784]).\n\tsize mismatch for hidden_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([400]).\n\tsize mismatch for hidden_layers.1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([200, 400]).\n\tsize mismatch for hidden_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([100, 200]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 128]) from checkpoint, the shape in current model is torch.Size([10, 100])."
     ]
    }
   ],
   "source": [
    "\n",
    "# Try this\n",
    "model = fc_model.Network(784, 10, [400, 200, 100])\n",
    "# This will throw an error because the tensor sizes are wrong!\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para contornar isso, é necessário salvar não apenas o state_dict, mas também informações sobre a arquitetura da rede. Assim, um dicionário chamado checkpoint é criado, que contém o tamanho da entrada, o tamanho da saída, as camadas intermediárias e o state_dict. Esse dicionário é então salvo usando torch.save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = {'input_size': 784,\n",
    "              'output_size': 10,\n",
    "              'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É fornecida uma função load_checkpoint para carregar o checkpoint e reconstruir o modelo com a mesma arquitetura usada durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = fc_model.Network(checkpoint['input_size'],\n",
    "                             checkpoint['output_size'],\n",
    "                             checkpoint['hidden_layers'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=400, bias=True)\n",
      "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
      "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = load_checkpoint('checkpoint.pth')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Portanto, o state_dict e o checkpoint são usados para salvar e carregar o estado treinado de uma rede neural, garantindo que a arquitetura do modelo seja reconstruída corretamente durante o carregamento. Isso é útil para reutilizar modelos treinados ou continuar o treinamento de onde parou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
